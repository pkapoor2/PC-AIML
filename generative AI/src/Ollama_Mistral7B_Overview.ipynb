{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34be8617",
   "metadata": {},
   "source": [
    "\n",
    "# Overview of Ollama & Demo with Mistral 7B\n",
    "\n",
    "This notebook introduces **Ollama**, a platform for running large language models **locally**, and gives a hands-on demo using **Mistral 7B**, a lightweight but capable open-weight model.\n",
    "\n",
    "### Outline\n",
    "1. What is Ollama? Motivation & features\n",
    "2. Architecture & model ecosystem\n",
    "3. Installing & setup\n",
    "4. Using Ollama via Python / shell\n",
    "5. Demo: prompt + evaluation\n",
    "6. Limitations, best practices, and extensions\n",
    "\n",
    "*This notebook can be run on a laptop with adequate resources (see instructions below).* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4dd0d0",
   "metadata": {},
   "source": [
    "\n",
    "## 1. What is Ollama?\n",
    "\n",
    "- **Ollama** is an open framework for running large language models **locally**, avoiding dependence on cloud APIs.\n",
    "- **Motivations**:\n",
    "  - Privacy & data control: your data never leaves your machine.\n",
    "  - Lower latency and more control over inference.\n",
    "  - No per-token API cost once the model is downloaded.\n",
    "  - Full experimentation freedom: introspection, custom fine-tuning, etc.\n",
    "- Recent improvements: support for new architectures, improved performance and quantization.\n",
    "- Includes a public model registry with many popular open-weight models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a3efbd",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Model ecosystem in Ollama\n",
    "\n",
    "In the Ollama model registry you will find many models from small to large:\n",
    "\n",
    "- **Mistral 7B** — a state-of-the-art open model with 7 billion parameters.\n",
    "- **Phi-4**, **OLMo 2**, and others.\n",
    "\n",
    "We will use **Mistral 7B**, which provides a good balance of performance and resource requirements for laptop setups when quantized to 4-bit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e230c462",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Installing & setup\n",
    "\n",
    "Run the following shell commands (Linux / macOS). For Windows, use WSL or the Windows installer from the Ollama website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a01265",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install Ollama (if not installed)\n",
    "#!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Check version in terminal\n",
    "#ollama --version\n",
    "\n",
    "# Pull the Mistral 7B model\n",
    "#!ollama pull mistral\n",
    "\n",
    "# (Optional) show model info in terminal\n",
    "#ollama show mistral\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ebcc32",
   "metadata": {},
   "source": [
    "\n",
    "The quantized model is roughly 4–6 GB and will run on laptops with at least:\n",
    "- Apple Silicon M1/M2/M3 with ≥16 GB unified memory, or\n",
    "- NVIDIA GPU with ≥8 GB VRAM.\n",
    "\n",
    "CPU-only laptops can also run it but will be slower.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaf7c7a",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Using Ollama via Python (and shell)\n",
    "\n",
    "Ollama can be used from the command line or from Python using `subprocess` or its HTTP API.\n",
    "\n",
    "### CLI chat\n",
    "```bash\n",
    "ollama run mistral \"Tell me a short story about a robot and a cat.\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62233148",
   "metadata": {},
   "source": [
    "### Ollama HTTP API\n",
    "Although you can use a Python wrapper in a subprocess to call ollama with a prompt, the preferred method is using the API to reduce latency.\n",
    "\n",
    "> NOTE: Before runnnig the next cell, open a terminal and run \n",
    "```bash\n",
    "ollama serve \n",
    "```\n",
    ">to start the HTTP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2623fe38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='white-space: pre-wrap; word-wrap: break-word;'> In the heart of a bustling city, a curious robot named B-345 found an unexpected companion - a mischievous calico cat named Whiskers. While B-345 was designed to maintain order, Whiskers brought chaos, leaping onto B-345's cold metal frame and knocking sensors askew. Initially alarmed by the interruption, B-345 soon found itself warming to Whiskers' playful antics. One day, a power outage plunged the city into darkness, and it was Whiskers who guided B-345 through the unfamiliar, lit paths of the apartment complex. In that moment, a bond formed between the robot and the cat, proving that even in a world of cold precision, there is always room for the soft warmth of friendship.</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def ollama_chat(prompt: str, model: str = \"mistral\"):\n",
    "    \"\"\"\n",
    "    Run a prompt through the specified model using the current Ollama CLI syntax.\n",
    "    \"\"\"\n",
    "    r = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\"model\": model, \"prompt\": prompt, \"stream\": False},\n",
    "    )\n",
    "    return r.json()[\"response\"]\n",
    "\n",
    "# Example usage\n",
    "text = ollama_chat(\"Tell me a short story about a robot and a cat. One paragraph only\")\n",
    "\n",
    "# display with wrapping inside a <pre> block\n",
    "display(HTML(f\"<pre style='white-space: pre-wrap; word-wrap: break-word;'>{text}</pre>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7fa16f",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Demo: Prompting & Evaluation\n",
    "\n",
    "Below we send multiple prompts and view outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a22fc1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Prompt:</b> Write a short, whimsical poem about a moonlit forest.<br><pre style='white-space: pre-wrap; word-wrap: break-word;'> In the hush of night, under a cloak of silver light,\n",
       "\n",
       "A moonlit forest whispers its dreams so soft and slight.\n",
       "\n",
       "The owls hoot and sing in harmonious unite,\n",
       "\n",
       "As twilight dances with the stars' radiant sight.\n",
       "\n",
       "Shadowy leaves sway gently, like ballerinas,\n",
       "\n",
       "In a ballet of breeze, they perform their quiet drifters' trance.\n",
       "\n",
       "The fireflies begin to waltz, their tiny lights display,\n",
       "\n",
       "A mesmerizing symphony in the moonlit ballet.\n",
       "\n",
       "Through branches overhead, they spin and twirl with grace,\n",
       "\n",
       "Celebrating life in this mystical place.\n",
       "\n",
       "Deer tread softly on leaves that crunch underfoot,\n",
       "\n",
       "Their antlers shimmering, a silvered root.\n",
       "\n",
       "Wildflowers bloom in the moon's soft beam,\n",
       "\n",
       "An enchanting tapestry, woven by Nature's dream.\n",
       "\n",
       "In the heart of this forest, secrets are kept,\n",
       "\n",
       "Where time seems to pause and life takes a leap.\n",
       "\n",
       "A night of tranquility, where magic is replete,\n",
       "\n",
       "In this moonlit forest, dreams take flight.</pre><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Prompt:</b> Explain in simple terms how backpropagation works in neural networks.<br><pre style='white-space: pre-wrap; word-wrap: break-word;'> Backpropagation is a method used to train artificial neural networks (ANNs). It's a process that helps the network learn and improve its performance on a task, like recognizing images or understanding language.\n",
       "\n",
       "Here's a simplified explanation:\n",
       "\n",
       "1. **Forward Propagation**: First, an input is fed into the network. The input travels through each layer of the network (usually multiple hidden layers), with each neuron applying its weights and bias before passing the result to the next neuron. This process continues until it reaches the output layer, where a prediction or answer is made.\n",
       "\n",
       "2. **Calculate Loss**: The output from the network is compared to the desired output, and the difference between the two (called the \"loss\") is calculated. A lower loss means the network's prediction is closer to the correct answer.\n",
       "\n",
       "3. **Backward Propagation**: The loss is then propagated backward through the network. This means starting from the output layer, the error or gradient of the loss is calculated for each neuron and passed back to the previous layers. This helps us understand how much each connection in the network contributed to the overall error.\n",
       "\n",
       "4. **Update Weights**: Finally, based on the gradients calculated during backward propagation, the weights (and sometimes biases) of the connections are adjusted to reduce the loss. This process is repeated multiple times, with each iteration called an \"epoch\". The goal is to find a set of weights that minimizes the loss and makes the network's predictions as accurate as possible.\n",
       "\n",
       "In summary, backpropagation helps neural networks learn by adjusting their internal connections based on how well they perform on a task. It's a key component of deep learning and has revolutionized many fields by enabling computers to learn complex patterns from data.</pre><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Prompt:</b> Given the sentence: 'The cat sat on the mat.', produce an alternative sentence with same meaning but different structure.<br><pre style='white-space: pre-wrap; word-wrap: break-word;'> \"On the mat, the cat was seated.\" or \"Seated on the mat is the cat.\"</pre><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "prompts = [\n",
    "    \"Write a short, whimsical poem about a moonlit forest.\",\n",
    "    \"Explain in simple terms how backpropagation works in neural networks.\",\n",
    "    \"Given the sentence: 'The cat sat on the mat.', produce an alternative sentence with same meaning but different structure.\"\n",
    "]\n",
    "\n",
    "for p in prompts:\n",
    "    # POST request to the local Ollama server\n",
    "    r = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\"model\": \"mistral\", \"prompt\": p, \"stream\": False},\n",
    "    )\n",
    "    out = r.json()[\"response\"]\n",
    "\n",
    "    # Show the prompt and the wrapped response\n",
    "    display(HTML(\n",
    "        f\"<b>Prompt:</b> {p}<br>\"\n",
    "        f\"<pre style='white-space: pre-wrap; word-wrap: break-word;'>{out}</pre>\"\n",
    "        \"<hr>\"\n",
    "    ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b06a6",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluation ideas\n",
    "- Compare outputs from Mistral 7B with other models (if available) to observe differences in coherence and creativity.\n",
    "- Critique hallucinations or factual errors.\n",
    "- Try different prompting styles (formal, casual, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d9db9",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Limitations, best practices, and extensions\n",
    "\n",
    "**Limitations / challenges**\n",
    "- Laptop resource constraints (especially CPU-only).\n",
    "- Quantization trade-offs: aggressive quantization may reduce quality.\n",
    "- Latency on long prompts.\n",
    "\n",
    "**Best practices**\n",
    "- Use shorter context windows when possible.\n",
    "- Adjust temperature / top_p to control creativity.\n",
    "- Explore chain-of-thought prompting to improve reasoning.\n",
    "\n",
    "**Extensions**\n",
    "- Experiment with tool-calling or retrieval-augmented generation.\n",
    "- Fine-tune smaller adapters for specific domains.\n",
    "- Combine with external knowledge sources for RAG pipelines.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
