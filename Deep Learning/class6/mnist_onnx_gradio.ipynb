{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb56587f",
   "metadata": {},
   "source": [
    "# **MNIST CNN to ONNX: Can I guess the digit you drew?**\n",
    "\n",
    "This notebook trains a tiny CNN on **MNIST**, **exports to ONNX** (the Open Neural Network eXchange), and serves a **Gradio** canvas app that runs **inference with ONNX Runtime** (no PyTorch in the serving path).\n",
    "\n",
    "**Flow**  \n",
    "1. Install deps (PyTorch, Torchvision, ONNX, ONNX Runtime, Gradio)  \n",
    "2. Train small CNN (few epochs) **or** skip if `models/mnist_cnn.onnx` already exists  \n",
    "3. Export to `models/mnist_cnn.onnx` (+ optional int8 quantization)  \n",
    "4. Launch Gradio app with a large canvas and **MNIST-style preprocessing** (center/scale/pad) and a big upscaled 28×28 preview\n",
    "\n",
    "**Key Components**\n",
    "1. Model Training: Trains a small CNN on MNIST (or skips if ONNX model exists)\n",
    "2. ONNX Export: Converts PyTorch model to ONNX format for deployment\n",
    "3. Interactive Interface: Gradio app with drawing canvas for digit recognition\n",
    "4. MNIST-style Preprocessing: Converts user drawings to 28x28 format matching training data\n",
    "\n",
    "**Features**\n",
    "- Large drawing canvas (560x560) with brush/eraser tools\n",
    "- Automatic preprocessing (invert, crop, resize, blur, center-of-mass alignment)\n",
    "- Real-time inference using ONNX Runtime (no PyTorch dependency)\n",
    "- Confidence scores and probability distribution display\n",
    "- Adjustable preprocessing parameters (threshold, blur)\n",
    "- Optional int8 quantization for model compression, faster inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e0915b",
   "metadata": {},
   "source": [
    "## 0) Install/verify dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1908f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If your environment already has these, this cell will be quick. If versions update, restart kernel.\n",
    "%pip -q install --upgrade torch torchvision gradio onnx onnxruntime ml-dtypes tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5feccf29",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (894336283.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip uninstall onnxruntime\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip uninstall onnxruntime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b9801",
   "metadata": {},
   "source": [
    "## 1) Imports and setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2fc9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import onnx, onnxruntime as ort\n",
    "\n",
    "from PIL import Image, ImageOps, ImageFilter, ImageChops\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(0); np.random.seed(0); torch.manual_seed(0)\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "MODELS_DIR = Path(\"models\"); MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ONNX_PATH = MODELS_DIR / \"mnist_cnn.onnx\"\n",
    "META_PATH = MODELS_DIR / \"meta.json\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7820abd",
   "metadata": {},
   "source": [
    "## 2) Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We'll use augmentations to better match mouse-drawn digits\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "train_ds = datasets.MNIST(root=DATA_DIR, train=True, download=True, transform=train_transform)\n",
    "test_ds  = datasets.MNIST(root=DATA_DIR, train=False, download=True, transform=test_transform)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "len(train_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c4631",
   "metadata": {},
   "source": [
    "## 3) Model definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220179a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    '''Assume input is MNIST 1x28x28 (channel, height, width), in PyTorch NCHW format (batch,channels,height,width)'''\n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # 3x3 magnifying glass\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # downsample 28 to 14\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # 3x3 magnifying glass\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # downsample 14 to 7\n",
    "            nn.Flatten(),  # flatten 64x7x7 to 3136 vector of learned features\n",
    "            nn.Linear(64 * 7 * 7, 128), # build feature map of learned features to class scores\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes),   # output layer, one score per class, produces 10 logits, no activation\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdcdfff",
   "metadata": {},
   "source": [
    "## 4) Training utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for x, y in tqdm(loader, desc=\"train\", leave=False):   # tqdm = progress bars\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running += loss.item() * x.size(0)\n",
    "    return running / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()   # disable gradient calculation for faster evaluation, pure inference\n",
    "def evaluate_acc(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in tqdm(loader, desc=\"eval\", leave=False):   # tqdm = progress bars\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc949d9d",
   "metadata": {},
   "source": [
    "## 5) Train (or skip if ONNX already exists and you want to reuse it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39f6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SKIP_TRAINING_IF_ONNX_EXISTS = True\n",
    "\n",
    "if SKIP_TRAINING_IF_ONNX_EXISTS and ONNX_PATH.exists():\n",
    "    print(\"ONNX model already exists; skipping training. Delete it to retrain.\")\n",
    "else:\n",
    "    EPOCHS = 5   # tune for accuracy/speed tradeoff\n",
    "    LR = 1e-3    # small learning rate for stability, tune for speed/accuracy tradeoff\n",
    "\n",
    "    model = SmallCNN().to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        acc = evaluate_acc(model, test_loader, DEVICE)\n",
    "        print(f\"Epoch {epoch}: loss={loss:.4f} | acc={acc:.4f}\")\n",
    "        best_acc = max(best_acc, acc)\n",
    "\n",
    "    # Save PyTorch weights temporarily for export\n",
    "    torch_ckpt_path = MODELS_DIR / \"mnist_cnn.pt\"\n",
    "    torch.save(model.state_dict(), torch_ckpt_path)\n",
    "    with open(META_PATH, \"w\") as f:\n",
    "        json.dump({\"num_classes\": 10, \"normalize_mean\": 0.1307, \"normalize_std\": 0.3081}, f)\n",
    "    print(\"Training complete. Best test acc:\", best_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d935f09",
   "metadata": {},
   "source": [
    "## 6) Export to ONNX (if not present)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc9954",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not ONNX_PATH.exists():\n",
    "    # Rebuild model and load weights just for a clean export step\n",
    "    model = SmallCNN()\n",
    "    torch_ckpt_path = MODELS_DIR / \"mnist_cnn.pt\"\n",
    "    assert torch_ckpt_path.exists(), \"No checkpoint for export—set SKIP_TRAINING_IF_ONNX_EXISTS=False and rerun training.\"\n",
    "    model.load_state_dict(torch.load(torch_ckpt_path, map_location=\"cpu\"))\n",
    "    model.eval()\n",
    "\n",
    "    dummy = torch.randn(1, 1, 28, 28)  # NCHW\n",
    "    torch.onnx.export(\n",
    "        model, dummy, ONNX_PATH.as_posix(),\n",
    "        input_names=[\"input\"], output_names=[\"logits\"],\n",
    "        dynamic_axes={\"input\": {0: \"batch\"}, \"logits\": {0: \"batch\"}},\n",
    "        opset_version=13\n",
    "    )\n",
    "    print(\"Exported:\", ONNX_PATH.resolve())\n",
    "else:\n",
    "    print(\"Using existing:\", ONNX_PATH.resolve())\n",
    "\n",
    "# Validate ONNX model\n",
    "m = onnx.load(ONNX_PATH.as_posix())\n",
    "onnx.checker.check_model(m)\n",
    "print(\"ONNX model is valid.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9748e0ab",
   "metadata": {},
   "source": [
    "## 7) Prepare ONNX Runtime session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9ae084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sess = ort.InferenceSession(ONNX_PATH.as_posix(), providers=[\"CPUExecutionProvider\"])\n",
    "input_name = sess.get_inputs()[0].name\n",
    "output_name = sess.get_outputs()[0].name\n",
    "\n",
    "# Load meta\n",
    "with open(META_PATH) as f:\n",
    "    meta = json.load(f)\n",
    "mean, std = meta[\"normalize_mean\"], meta[\"normalize_std\"]\n",
    "\n",
    "# Preprocess to model tensor\n",
    "to_tensor = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((mean,), (std,)),\n",
    "])\n",
    "\n",
    "def onnx_predict_probs_from_pil(pil_img: Image.Image):\n",
    "    x = to_tensor(pil_img).unsqueeze(0).numpy()  # NCHW float32\n",
    "    logits = sess.run([output_name], {input_name: x})[0]  # (N, 10)\n",
    "    # softmax\n",
    "    logits = logits - logits.max(axis=1, keepdims=True)\n",
    "    exp = np.exp(logits)\n",
    "    probs = exp / exp.sum(axis=1, keepdims=True)\n",
    "    p = probs[0]\n",
    "    return {str(i): float(p[i]) for i in range(10)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816497ce",
   "metadata": {},
   "source": [
    "## 8) MNIST-style preprocessing for drawings (center/scale/pad, COM recenter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _center_of_mass_shift(im: Image.Image):\n",
    "    arr = np.array(im, dtype=np.float32)\n",
    "    total = arr.sum()\n",
    "    if total <= 0: return 0, 0\n",
    "    ys, xs = np.indices(arr.shape)\n",
    "    cy = (ys * arr).sum() / total\n",
    "    cx = (xs * arr).sum() / total\n",
    "    return int(round(arr.shape[1]/2 - cx)), int(round(arr.shape[0]/2 - cy))\n",
    "\n",
    "def preprocess_mnist_style(pil: Image.Image, threshold=10, blur=0.5) -> Image.Image:\n",
    "    img = pil.convert(\"L\")\n",
    "    if np.array(img).mean() > 127:  # invert if white background\n",
    "        img = ImageOps.invert(img)\n",
    "    arr = np.array(img, dtype=np.float32)\n",
    "    if arr.max() > arr.min():\n",
    "        arr = (arr - arr.min()) / (arr.max() - arr.min()) * 255.0\n",
    "    arr_u8 = arr.astype(\"uint8\")\n",
    "    mask = arr_u8 > int(threshold)\n",
    "    if mask.any():\n",
    "        ys, xs = np.where(mask)\n",
    "        y0, y1 = max(0, ys.min()-2), min(arr_u8.shape[0], ys.max()+3)\n",
    "        x0, x1 = max(0, xs.min()-2), min(arr_u8.shape[1], xs.max()+3)\n",
    "        img = Image.fromarray(arr_u8[y0:y1, x0:x1])\n",
    "    else:\n",
    "        img = Image.fromarray(arr_u8)\n",
    "    w, h = img.size\n",
    "    if max(w, h) == 0:\n",
    "        return Image.new(\"L\", (28, 28), 0)\n",
    "    scale = 20 / max(w, h)\n",
    "    nw, nh = max(1, int(round(w*scale))), max(1, int(round(h*scale)))\n",
    "    img = img.resize((nw, nh), Image.BILINEAR)\n",
    "    canvas = Image.new(\"L\", (28, 28), 0)\n",
    "    canvas.paste(img, ((28-nw)//2, (28-nh)//2))\n",
    "    if blur and blur > 0:\n",
    "        canvas = canvas.filter(ImageFilter.GaussianBlur(radius=float(blur)))\n",
    "    dx, dy = _center_of_mass_shift(canvas)\n",
    "    return ImageChops.offset(canvas, dx, dy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2999b29b",
   "metadata": {},
   "source": [
    "## 9) Gradio UI (ImageEditor, big canvas, upscaled preview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr\n",
    "\n",
    "def top_pred(probs_dict):\n",
    "    label, conf = max(probs_dict.items(), key=lambda kv: kv[1])\n",
    "    return f\"Prediction: {label} (confidence {conf:.2%})\"\n",
    "\n",
    "def _to_pil_from_editor(value):\n",
    "    if value is None: return None\n",
    "    if isinstance(value, dict):      # ImageEditor (type=\"pil\")\n",
    "        return value.get(\"composite\") or value.get(\"background\")\n",
    "    if isinstance(value, Image.Image):\n",
    "        return value\n",
    "    if isinstance(value, np.ndarray):\n",
    "        return Image.fromarray(value.astype(\"uint8\"))\n",
    "    return Image.open(value)\n",
    "\n",
    "def predict(editor_value, threshold, blur, preview_px):\n",
    "    raw = _to_pil_from_editor(editor_value)\n",
    "    if raw is None:\n",
    "        return \"Please draw or upload a digit.\", {}, None\n",
    "    processed = preprocess_mnist_style(raw, threshold=threshold, blur=blur)\n",
    "    probs = onnx_predict_probs_from_pil(processed)\n",
    "    preview = processed.resize((int(preview_px), int(preview_px)), Image.NEAREST)\n",
    "    return top_pred(probs), probs, preview\n",
    "\n",
    "\"\"\"\n",
    "Gradio UI (Blocks) for MNIST → ONNX Runtime demo.\n",
    "\n",
    "Purpose\n",
    "-------\n",
    "Provide an interactive, notebook-native UI where students draw a digit,\n",
    "inspect the exact 28x28 tensor fed to the model, and run inference using\n",
    "ONNX Runtime.\n",
    "\n",
    "How it works\n",
    "------------\n",
    "1) Students draw/upload in the left ImageEditor canvas.\n",
    "2) We preprocess to MNIST style: grayscale → invert (if needed) →\n",
    "   crop to digit via threshold → resize longest side to 20 px →\n",
    "   pad to 28x28 → light blur → center-of-mass recenter.\n",
    "3) The ONNX model sees ONLY the 28x28 tensor; the preview simply upscales it.\n",
    "\n",
    "Controls\n",
    "--------\n",
    "- Preprocess threshold (0-50):\n",
    "    Sets the pixel cutoff for detecting the digit's bounding box.\n",
    "    • Increase to ignore faint specks/noise.\n",
    "    • Decrease if thin/light strokes get clipped.\n",
    "\n",
    "- Preprocess blur (σ (sigma), 0.0-1.5):\n",
    "    Small Gaussian blur after resizing helps antialiased mouse strokes.\n",
    "    • Typical sweet spot: 0.4-0.8.\n",
    "    • Too high will over-smooth skinny digits.\n",
    "\n",
    "- Preview size (px):\n",
    "    Upscales the 28x28 model input for inspection only.\n",
    "    • Does NOT change the model's input resolution.\n",
    "\n",
    "UI pieces\n",
    "---------\n",
    "- ImageEditor (left): draw with brush/eraser; large canvas for comfort.\n",
    "- Preview (right): the 28x28 MNIST-style input, upscaled with nearest-neighbor.\n",
    "- Predict button: runs preprocessing + ONNX inference and displays\n",
    "  (a) the top prediction + confidence and (b) full class probabilities (0-9).\n",
    "\n",
    "Tips\n",
    "----\n",
    "• Use a thicker brush and center the digit; adjust threshold/blur if predictions look off.\n",
    "• The preview is your ground truth for what the model actually “sees.”\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(title=\"MNIST (ONNX Runtime)\") as demo:\n",
    "    gr.Markdown(\"### Draw a digit (0-9), then **Predict** — served via **ONNX Runtime**\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            canvas = gr.ImageEditor(\n",
    "                image_mode=\"L\", type=\"pil\",\n",
    "                sources=[\"upload\", \"clipboard\", \"webcam\"],\n",
    "                brush=gr.Brush(default_size=36),\n",
    "                eraser=gr.Eraser(default_size=36),\n",
    "                canvas_size=(560, 560),\n",
    "                height=640, width=640, label=\"Canvas / Upload\",\n",
    "            )\n",
    "            threshold = gr.Slider(0, 50, value=10, step=1, label=\"Preprocess threshold\")\n",
    "            blur = gr.Slider(0.0, 1.5, value=0.5, step=0.1, label=\"Preprocess blur (σ)\")\n",
    "            preview_px = gr.Slider(320, 768, value=560, step=80, label=\"Preview size (px)\")\n",
    "        processed_preview = gr.Image(label=\"Model input (28x28, upscaled)\")\n",
    "\n",
    "    btn = gr.Button(\"Predict (ONNX)\", variant=\"primary\")\n",
    "    pred_text = gr.Markdown()\n",
    "    probs_json = gr.JSON(label=\"Class probabilities\")\n",
    "\n",
    "    btn.click(\n",
    "        predict,\n",
    "        inputs=[canvas, threshold, blur, preview_px],\n",
    "        outputs=[pred_text, probs_json, processed_preview],\n",
    "    )\n",
    "\n",
    "demo.launch(share=False, inline=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6926ed3a",
   "metadata": {},
   "source": [
    "\n",
    "## Optional: Dynamic Quantization\n",
    "Shrink the ONNX model and sometimes speed up CPU inference. Use this ONNYX model when you are using CPU only, want a small download and snappy startup, or plan to ship the .onnx to a web/backend service.\n",
    "\n",
    "### What you get\n",
    "- Smaller file: weights go from float32 (4 bytes) → int8 (1 byte).\n",
    "    - Our ~421k-param model is ~1.7 MB in fp32; INT8 drops ~4× (≈0.4–0.5 MB).\n",
    "- Faster CPU inference: ONNX Runtime can use vectorized INT8 kernels (e.g., AVX2/VNNI), which can beat fp32, especially on laptops/servers without powerful GPUs.\n",
    "- Simpler deployment: smaller artifacts = faster downloads, lighter containers.\n",
    "\n",
    "### What you give up\n",
    "- Tiny accuracy dip is possible (usually negligible for MNIST).\n",
    "- Op coverage: all layers must be supported by ORT’s quantized kernels (your CNN is fine).\n",
    "- Speedups are hardware-dependent: newer CPUs benefit more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "    INT8_PATH = (MODELS_DIR / \"mnist_cnn.int8.onnx\").as_posix()\n",
    "    quantize_dynamic(\n",
    "        model_input=ONNX_PATH.as_posix(),\n",
    "        model_output=INT8_PATH,\n",
    "        per_channel=False,\n",
    "        reduce_range=False,\n",
    "        weight_type=QuantType.QInt8,\n",
    "    )\n",
    "    print(\"Quantized model written to:\", INT8_PATH)\n",
    "except Exception as e:\n",
    "    print(\"Quantization not available:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
