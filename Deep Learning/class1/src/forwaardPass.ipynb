{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bba97dbc",
   "metadata": {},
   "source": [
    "# Single Hidden Layer Forward Pass Demo\n",
    "\n",
    "This notebook demonstrates a simple forward pass through a neural network with:\n",
    "- Input layer: 3 neurons\n",
    "- Hidden layer: 4 neurons (with ReLU activation)\n",
    "- Output layer: 2 neurons (with Sigmoid activation)\n",
    "\n",
    "We'll implement this step by step to understand how data flows through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7152c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "NumPy version: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fed53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -250, 250)))  # Clip to prevent overflow\n",
    "\n",
    "# Test the activation functions\n",
    "x_test = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"Input:\", x_test)\n",
    "print(\"ReLU output:\", relu(x_test))\n",
    "print(\"Sigmoid output:\", sigmoid(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture\n",
    "input_size = 3    # Number of input features\n",
    "hidden_size = 4   # Number of neurons in hidden layer\n",
    "output_size = 2   # Number of output neurons\n",
    "\n",
    "print(\"Network Architecture:\")\n",
    "print(f\"Input layer: {input_size} neurons\")\n",
    "print(f\"Hidden layer: {hidden_size} neurons (ReLU activation)\")\n",
    "print(f\"Output layer: {output_size} neurons (Sigmoid activation)\")\n",
    "\n",
    "# Initialize weights and biases randomly\n",
    "# Weights from input to hidden layer\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
    "b1 = np.random.randn(hidden_size) * 0.5\n",
    "\n",
    "# Weights from hidden to output layer\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
    "b2 = np.random.randn(output_size) * 0.5\n",
    "\n",
    "print(\"\\nWeight shapes:\")\n",
    "print(f\"W1 (input to hidden): {W1.shape}\")\n",
    "print(f\"b1 (hidden bias): {b1.shape}\")\n",
    "print(f\"W2 (hidden to output): {W2.shape}\")\n",
    "print(f\"b2 (output bias): {b2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample input data\n",
    "# Single sample (1 x 3)\n",
    "X_single = np.array([1.5, -0.5, 2.0])\n",
    "\n",
    "# Multiple samples (batch of 5 samples, each with 3 features)\n",
    "X_batch = np.array([\n",
    "    [1.5, -0.5, 2.0],\n",
    "    [0.5, 1.0, -1.5],\n",
    "    [-1.0, 0.5, 0.0],\n",
    "    [2.0, -1.0, 1.0],\n",
    "    [0.0, 2.0, -0.5]\n",
    "])\n",
    "\n",
    "print(\"Single sample input:\")\n",
    "print(f\"X_single shape: {X_single.shape}\")\n",
    "print(f\"X_single: {X_single}\")\n",
    "\n",
    "print(\"\\nBatch input:\")\n",
    "print(f\"X_batch shape: {X_batch.shape}\")\n",
    "print(f\"X_batch:\\n{X_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7630eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, W1, b1, W2, b2, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform forward pass through the network\n",
    "    \n",
    "    Parameters:\n",
    "    X: input data (batch_size, input_size) or (input_size,)\n",
    "    W1, b1: weights and bias for hidden layer\n",
    "    W2, b2: weights and bias for output layer\n",
    "    verbose: whether to print intermediate steps\n",
    "    \n",
    "    Returns:\n",
    "    output: final network output\n",
    "    hidden_output: hidden layer output (for visualization)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure X is 2D (add batch dimension if needed)\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(1, -1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=== Forward Pass Details ===\")\n",
    "        print(f\"Input X shape: {X.shape}\")\n",
    "        print(f\"Input X:\\n{X}\")\n",
    "    \n",
    "    # Step 1: Input to Hidden Layer\n",
    "    # Linear transformation: z1 = X @ W1 + b1\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStep 1 - Input to Hidden:\")\n",
    "        print(f\"z1 = X @ W1 + b1\")\n",
    "        print(f\"z1 shape: {z1.shape}\")\n",
    "        print(f\"z1 (before activation):\\n{z1}\")\n",
    "    \n",
    "    # Apply ReLU activation\n",
    "    hidden_output = relu(z1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Hidden layer output (after ReLU):\\n{hidden_output}\")\n",
    "    \n",
    "    # Step 2: Hidden to Output Layer\n",
    "    # Linear transformation: z2 = hidden_output @ W2 + b2\n",
    "    z2 = np.dot(hidden_output, W2) + b2\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStep 2 - Hidden to Output:\")\n",
    "        print(f\"z2 = hidden_output @ W2 + b2\")\n",
    "        print(f\"z2 shape: {z2.shape}\")\n",
    "        print(f\"z2 (before activation):\\n{z2}\")\n",
    "    \n",
    "    # Apply Sigmoid activation\n",
    "    output = sigmoid(z2)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Final output (after Sigmoid):\\n{output}\")\n",
    "        print(\"=== End Forward Pass ===\\n\")\n",
    "    \n",
    "    return output, hidden_output\n",
    "\n",
    "print(\"Forward pass function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff5cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1: Forward pass with a single sample (detailed)\n",
    "print(\"🔥 DEMO 1: Single Sample Forward Pass (Detailed)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "output_single, hidden_single = forward_pass(X_single, W1, b1, W2, b2, verbose=True)\n",
    "\n",
    "print(\"SUMMARY:\")\n",
    "print(f\"Input: {X_single}\")\n",
    "print(f\"Hidden layer output: {hidden_single.flatten()}\")\n",
    "print(f\"Final output: {output_single.flatten()}\")\n",
    "print(f\"Sum of outputs: {np.sum(output_single):.4f}\")  # For sigmoid, this doesn't need to be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: Forward pass with a batch of samples\n",
    "print(\"🔥 DEMO 2: Batch Forward Pass\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "output_batch, hidden_batch = forward_pass(X_batch, W1, b1, W2, b2, verbose=False)\n",
    "\n",
    "print(f\"Batch input shape: {X_batch.shape}\")\n",
    "print(f\"Hidden layer output shape: {hidden_batch.shape}\")\n",
    "print(f\"Final output shape: {output_batch.shape}\")\n",
    "print(\"\\nResults for each sample:\")\n",
    "for i in range(X_batch.shape[0]):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Input: {X_batch[i]}\")\n",
    "    print(f\"  Hidden: {hidden_batch[i]}\")\n",
    "    print(f\"  Output: {output_batch[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Network architecture and activations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Network weights W1 (input to hidden)\n",
    "im1 = axes[0, 0].imshow(W1.T, cmap='RdBu', aspect='auto')\n",
    "axes[0, 0].set_title('Weights W1 (Input → Hidden)')\n",
    "axes[0, 0].set_xlabel('Input Neurons')\n",
    "axes[0, 0].set_ylabel('Hidden Neurons')\n",
    "plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "# Plot 2: Network weights W2 (hidden to output)\n",
    "im2 = axes[0, 1].imshow(W2.T, cmap='RdBu', aspect='auto')\n",
    "axes[0, 1].set_title('Weights W2 (Hidden → Output)')\n",
    "axes[0, 1].set_xlabel('Hidden Neurons')\n",
    "axes[0, 1].set_ylabel('Output Neurons')\n",
    "plt.colorbar(im2, ax=axes[0, 1])\n",
    "\n",
    "# Plot 3: Hidden layer activations for batch\n",
    "im3 = axes[1, 0].imshow(hidden_batch.T, cmap='viridis', aspect='auto')\n",
    "axes[1, 0].set_title('Hidden Layer Activations (Batch)')\n",
    "axes[1, 0].set_xlabel('Sample Index')\n",
    "axes[1, 0].set_ylabel('Hidden Neurons')\n",
    "plt.colorbar(im3, ax=axes[1, 0])\n",
    "\n",
    "# Plot 4: Output activations for batch\n",
    "im4 = axes[1, 1].imshow(output_batch.T, cmap='plasma', aspect='auto')\n",
    "axes[1, 1].set_title('Output Activations (Batch)')\n",
    "axes[1, 1].set_xlabel('Sample Index')\n",
    "axes[1, 1].set_ylabel('Output Neurons')\n",
    "plt.colorbar(im4, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 Visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95bc2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 Interactive Experiment: Try your own inputs!\n",
    "print(\"🧪 EXPERIMENT: Try Different Inputs\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# You can modify these values to see how the network responds\n",
    "custom_input = np.array([2.0, -1.5, 0.5])\n",
    "print(f\"Testing with custom input: {custom_input}\")\n",
    "\n",
    "custom_output, custom_hidden = forward_pass(custom_input, W1, b1, W2, b2, verbose=True)\n",
    "\n",
    "# Show the effect of different input magnitudes\n",
    "print(\"\\n🔍 Effect of Input Magnitude:\")\n",
    "test_inputs = [\n",
    "    np.array([0.1, 0.1, 0.1]),    # Small values\n",
    "    np.array([1.0, 1.0, 1.0]),    # Medium values  \n",
    "    np.array([5.0, 5.0, 5.0]),    # Large values\n",
    "    np.array([-2.0, -2.0, -2.0])  # Negative values\n",
    "]\n",
    "\n",
    "for i, test_input in enumerate(test_inputs):\n",
    "    output, hidden = forward_pass(test_input, W1, b1, W2, b2, verbose=False)\n",
    "    print(f\"Input {i+1}: {test_input} → Output: {output.flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516085f0",
   "metadata": {},
   "source": [
    "## 📝 Key Concepts Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Forward Pass Process:**\n",
    "   - Input → Linear Transformation → Activation → Linear Transformation → Activation → Output\n",
    "   - Each layer transforms the data step by step\n",
    "\n",
    "2. **Matrix Operations:**\n",
    "   - Linear transformation: `z = X @ W + b`\n",
    "   - Works with both single samples and batches\n",
    "\n",
    "3. **Activation Functions:**\n",
    "   - **ReLU**: `max(0, x)` - introduces non-linearity, kills negative values\n",
    "   - **Sigmoid**: `1/(1 + e^(-x))` - squashes output to (0,1) range\n",
    "\n",
    "4. **Network Components:**\n",
    "   - **Weights (W)**: Learned parameters that determine transformations\n",
    "   - **Biases (b)**: Learned parameters that shift the activation\n",
    "   - **Activations**: Non-linear functions that enable complex mappings\n",
    "\n",
    "### Next Steps:\n",
    "- Try modifying the network architecture (different sizes)\n",
    "- Experiment with different activation functions\n",
    "- Learn about backpropagation to train the weights\n",
    "- Explore how to handle real datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
