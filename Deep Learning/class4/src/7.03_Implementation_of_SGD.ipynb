{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Af46ONne8Kok"
   },
   "source": [
    "# __Stochastic Gradient Descent (SGD)__\n",
    "- Stochastic gradient descent (SGD) is an optimization algorithm, commonly used in machine learning to train models. It is easier to fit into memory due to a single training sample being processed by the network.\n",
    "- It is computationally fast as only one sample is processed at a time. For larger datasets, it can converge faster as it causes updates to the parameters more frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2WN2sjS6tOz"
   },
   "source": [
    "## Steps to be followed:\n",
    "1. Import the required libraries\n",
    "2. Load the dataset\n",
    "3. Preprocess the data\n",
    "4. Initialize parameters\n",
    "5. Define the loss function\n",
    "6. Implement the SGD algorithm\n",
    "7. Train the model\n",
    "8. Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGJaVWRa9ztn"
   },
   "source": [
    "  ### Step 1: Import the required libraries\n",
    "\n",
    "  - It imports the necessary libraries and modules for data analysis and evaluation tasks.\n",
    "\n",
    "  - It specifically imports NumPy (for numerical operations), Pandas (for data manipulation), Matplotlib (for data visualization), and scikit-learn (for machine learning tasks) modules and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IqYyhHl_8Koo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLS7wopi-DOG"
   },
   "source": [
    "### Step 2: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7fatQBEK8Koq"
   },
   "outputs": [],
   "source": [
    "iris_data = load_iris()\n",
    "X, y = iris_data.data, iris_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqjlN8rJRgDH"
   },
   "source": [
    "**Observation**\n",
    "\n",
    "- The Iris dataset is successfully loaded. It contains 150 samples with 4 features each. The target variable has 3 classes representing different species of Iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vALXx_--4ma"
   },
   "source": [
    "### Step 3: Preprocess the data\n",
    "\n",
    "- One-hot encode the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_mu2tkIr8Koq"
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output = False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-k9s0EzKAY4c"
   },
   "source": [
    "**Observation**\n",
    "\n",
    "- The target variable y is one-hot encoded, transforming it from a single column of class labels to a matrix where each row is a one-hot encoded vector representing the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GP6YF4lYSe0d"
   },
   "source": [
    "- Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "d8M1ZpFvShfX"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpTFecMNSkCj"
   },
   "source": [
    "**Observation**\n",
    "\n",
    "- The dataset is split into training (80%) and testing (20%) sets. This separation ensures that we can evaluate the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIPIfhCQSrrS"
   },
   "source": [
    "- Standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4dE3YA-kSvAM"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gwC2zZMSxVF"
   },
   "source": [
    "**Observation**\n",
    "\n",
    "- The features are standardized to have zero mean and unit variance, which helps in faster convergence of the SGD algorithm and ensures that all features contribute equally to the gradient updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mf769SVZS9Fn"
   },
   "source": [
    "### Step 4: Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WcXapJLuTUdT"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "weights = np.random.randn(X_train_sc.shape[1], y_train.shape[1])\n",
    "bias = np.random.randn(y_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHpJ0GO4TYSl"
   },
   "source": [
    "**Observation**\n",
    "\n",
    "- The weights and bias are initialized randomly. This randomness can affect the starting point and convergence path of the algorithm. Setting a random seed ensures reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e5xPiD2Tf0_"
   },
   "source": [
    "### Step 5: Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "l_NZxP1QTjvX"
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    if z.ndim == 1:\n",
    "        z = z.reshape(1, -1)\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def compute_loss(X, y, weights, bias):\n",
    "    predictions = softmax(np.dot(X, weights) + bias)\n",
    "    loss = -np.mean(np.sum(y * np.log(predictions), axis=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBpcfNE6ToG9"
   },
   "source": [
    "**Observation**\n",
    "\n",
    "- The softmax function calculates probabilities for each class, ensuring that the sum of probabilities for each sample equals 1.\n",
    "- The cross-entropy loss function measures the difference between the predicted and actual distributions. It penalizes incorrect predictions more heavily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-JdXPwdTv6Q"
   },
   "source": [
    "### Step 6: Implement the SGD algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pENIsHafTnPb"
   },
   "outputs": [],
   "source": [
    "def sgd(X, y, weights, bias, learning_rate, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(X.shape[0]):\n",
    "            # Compute the prediction\n",
    "            z = np.dot(X[i], weights) + bias\n",
    "            prediction = softmax(z).flatten()\n",
    "\n",
    "            # Compute the error\n",
    "            error = prediction - y[i]\n",
    "\n",
    "            # Update the weights and bias\n",
    "            weights -= learning_rate * np.outer(X[i], error)\n",
    "            bias -= learning_rate * error\n",
    "\n",
    "        # Optionally, print the loss for each epoch\n",
    "        if epoch % 10 == 0:\n",
    "            loss = compute_loss(X, y, weights, bias)\n",
    "            print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cK9EBvEfT7es"
   },
   "source": [
    "**Observation**\n",
    "\n",
    "- The algorithm iteratively updates weights and bias for each sample, minimizing the loss function.\n",
    "- The loss is printed every 10 epochs to track the training progress.\n",
    "- The decreasing loss over epochs indicates that the model is learning and improving its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ammHNP7DUEZR"
   },
   "source": [
    "### Step 7: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 974,
     "status": "ok",
     "timestamp": 1719205969481,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "8gHkTsSOT6TW",
    "outputId": "d17763d1-6375-4608-ba7c-b98d82069a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.0880300581662026\n",
      "Epoch 10, Loss: 0.33942298320903863\n",
      "Epoch 20, Loss: 0.25877241037361626\n",
      "Epoch 30, Loss: 0.2152833378451731\n",
      "Epoch 40, Loss: 0.18748912695944972\n",
      "Epoch 50, Loss: 0.16804243546955017\n",
      "Epoch 60, Loss: 0.1536121423967302\n",
      "Epoch 70, Loss: 0.14244701115914368\n",
      "Epoch 80, Loss: 0.13353214357163423\n",
      "Epoch 90, Loss: 0.1262367431452422\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01  # Define learning rate\n",
    "epochs = 100          # Define number of epochs\n",
    "weights, bias = sgd(X_train_sc, y_train, weights, bias, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woOF8XYWUPum"
   },
   "source": [
    "**Observation**\n",
    "\n",
    "- The model is trained using the SGD function. As epochs increase, the loss typically decreases, showing that the model is learning and the parameters are being optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAhgzfppUWqp"
   },
   "source": [
    "### Step 8: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1719206005960,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "AjLmOduDUb8q",
    "outputId": "efb0f032-3682-47dd-b080-5f8953c8e99f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9666666666666667\n",
      "Testing Accuracy: 1.0\n",
      "Training Confusion Matrix:\n",
      " [[40  0  0]\n",
      " [ 0 38  3]\n",
      " [ 0  1 38]]\n",
      "Testing Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Training Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        40\n",
      "           1       0.97      0.93      0.95        41\n",
      "           2       0.93      0.97      0.95        39\n",
      "\n",
      "    accuracy                           0.97       120\n",
      "   macro avg       0.97      0.97      0.97       120\n",
      "weighted avg       0.97      0.97      0.97       120\n",
      "\n",
      "Testing Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "def predict(X, weights, bias):\n",
    "    predictions = softmax(np.dot(X, weights) + bias)\n",
    "    return np.argmax(predictions, axis=1)\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = predict(X_train_sc, weights, bias)\n",
    "y_test_pred = predict(X_test_sc, weights, bias)\n",
    "\n",
    "# Convert one-hot encoded targets back to labels for evaluation\n",
    "y_train_true = np.argmax(y_train, axis=1)\n",
    "y_test_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Compute accuracy\n",
    "train_accuracy = accuracy_score(y_train_true, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test_true, y_test_pred)\n",
    "\n",
    "# Compute other evaluation metrics\n",
    "train_conf_matrix = confusion_matrix(y_train_true, y_train_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test_true, y_test_pred)\n",
    "train_class_report = classification_report(y_train_true, y_train_pred)\n",
    "test_class_report = classification_report(y_test_true, y_test_pred)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Testing Accuracy: {test_accuracy}')\n",
    "print(\"Training Confusion Matrix:\\n\", train_conf_matrix)\n",
    "print(\"Testing Confusion Matrix:\\n\", test_conf_matrix)\n",
    "print(\"Training Classification Report:\\n\", train_class_report)\n",
    "print(\"Testing Classification Report:\\n\", test_class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unhK8RE3Ug2z"
   },
   "source": [
    "**Observation**\n",
    "\n",
    "- The predictions are made on the training and testing sets.\n",
    "- The accuracy scores provide a measure of the model's performance.\n",
    "- High training accuracy indicates the model fits the training data well.\n",
    "- High testing accuracy suggests the model generalizes well to unseen data.\n",
    "- Using stochastic gradient descent (SGD) for training a classification model on the Iris dataset demonstrates the effectiveness of the algorithm. The step-by-step observations show the model's learning process, starting from data preprocessing, parameter initialization, loss computation, iterative parameter updates, and finally evaluating the model's performance."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
